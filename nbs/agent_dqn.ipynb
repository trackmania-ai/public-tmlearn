{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0df09-6d2b-415f-b285-70502df6a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp agent.dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d716fba-3718-4849-8ca0-e63d02c54a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from functools import cached_property, partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from fastai.optimizer import Adam\n",
    "from fastcore.basics import class2attr, store_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712657d9-30cc-4930-bad4-3fe358645fde",
   "metadata": {},
   "source": [
    "# DQN Agent\n",
    "> Agent allowing to pick an action and fit one batch for training using DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcb34f-a4a3-404e-961f-ca7af4651398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class Exploration:\n",
    "    def __init__(self):\n",
    "        self.action_tier = 1.0\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return class2attr(self, \"Exploration\")\n",
    "\n",
    "    def __call__(self, action_values):\n",
    "        n_action = len(action_values)\n",
    "        argsort = action_values.argsort()\n",
    "        argmax = argsort[-1].item()\n",
    "        rank = argsort.argsort().double()\n",
    "        action = self.select_action(action_values, n_action, argmax, rank)\n",
    "        self.action_tier = (rank[action] / (len(rank) - 1)).item()\n",
    "        return action\n",
    "\n",
    "    def select_action(self, action_values, n_action, argmax, rank):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def state_dict(self):\n",
    "        return dict(exploration=self.name, action_tier=self.action_tier)\n",
    "\n",
    "\n",
    "class EpsilonGreedyExploration(Exploration):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "\n",
    "    def select_action(self, action_values, n_action, argmax, rank):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(n_action)\n",
    "        return argmax\n",
    "\n",
    "\n",
    "class ResmaxExploration(Exploration):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        self.eta_inv = eta**-1\n",
    "\n",
    "    def select_action(self, action_values, n_action, argmax, rank):\n",
    "        p = 1 / (n_action + self.eta_inv * (action_values[argmax] - action_values))\n",
    "        p = p.double()\n",
    "        p[argmax] += 1 - p.sum()\n",
    "        return np.random.choice(n_action, p=p)\n",
    "\n",
    "\n",
    "class RankSoftmaxExploration(Exploration):\n",
    "    def __init__(self, tau=1.0):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "\n",
    "    def select_action(self, action_values, n_action, argmax, rank):\n",
    "        p = (rank**self.tau).softmax(0)\n",
    "        return np.random.choice(n_action, p=p)\n",
    "\n",
    "\n",
    "class NoExploration(Exploration):\n",
    "    def select_action(self, action_values, n_action, argmax, rank):\n",
    "        return argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76f01c-e095-4b15-8e7c-4def2febfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class DqnAgent:\n",
    "    tags = [\"DQN\"]\n",
    "    \"\"\" Require use of DiscreteActionHandler in Tmenv \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        exploration=EpsilonGreedyExploration(),\n",
    "        device=\"cpu\",\n",
    "        gamma=0.99,\n",
    "        update_target_every=320,\n",
    "        optimizer_fn=partial(Adam, lr=1e-5, wd=1e-5),\n",
    "        loss_fn=torch.nn.SmoothL1Loss(reduction=\"none\"),\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.net_online = model.to(device=device)\n",
    "        self.net_online.share_memory()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, state, evaluation=False):\n",
    "        self.net_online.eval()\n",
    "        action_values = self.net_online(state).squeeze().cpu()\n",
    "        if evaluation:\n",
    "            return action_values.argmax()\n",
    "        return self.exploration(action_values)\n",
    "\n",
    "    @cached_property\n",
    "    def net_target(self):\n",
    "        net_target = deepcopy(self.net_online).eval()\n",
    "        for p in net_target.parameters():\n",
    "            p.requires_grad = False\n",
    "        return net_target\n",
    "\n",
    "    @cached_property\n",
    "    def nb_batch(self):\n",
    "        return 0\n",
    "\n",
    "    @cached_property\n",
    "    def optimizer(self):\n",
    "        return self.optimizer_fn(self.net_online.parameters())\n",
    "\n",
    "    @cached_property\n",
    "    def scaler(self):\n",
    "        return torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def _td_estimate(self, state, action):\n",
    "        return self.net_online(state)[np.arange(0, len(action)), action]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net_online(next_state)\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net_target(next_state)[\n",
    "            np.arange(0, len(best_action)), best_action\n",
    "        ]\n",
    "        return (\n",
    "            torch.tensor(reward, device=self.device)\n",
    "            + (1 - torch.tensor(done, device=self.device, dtype=torch.float))\n",
    "            * self.gamma\n",
    "            * next_Q\n",
    "        ).float()\n",
    "\n",
    "    def _sync_Q_target(self):\n",
    "        self.net_target.load_state_dict(self.net_online.state_dict())\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.optimizer.set_hyper(\"lr\", lr)\n",
    "\n",
    "    def fit_one_batch(self, batch, weights):\n",
    "        state, next_state, action, reward, done = zip(*batch)\n",
    "\n",
    "        self.net_online.train()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            td_est = self._td_estimate(state, action)\n",
    "            td_tgt = self._td_target(reward, next_state, done)\n",
    "            loss = (self.loss_fn(td_est, td_tgt) * weights).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        self.nb_batch += 1\n",
    "        if self.nb_batch % self.update_target_every == 0:\n",
    "            self._sync_Q_target()\n",
    "\n",
    "        errors = torch.abs(td_est - td_tgt).detach()\n",
    "        metrics = dict(\n",
    "            mean_errors=errors.mean().item(),\n",
    "            loss=loss.item(),\n",
    "            lr=self.optimizer.param_groups[0][\"lr\"],\n",
    "        )\n",
    "\n",
    "        return errors, metrics\n",
    "\n",
    "    def state_dict(self):\n",
    "        return dict(\n",
    "            net_online=self.net_online.state_dict(),\n",
    "        )\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.net_online.load_state_dict(state_dict[\"net_online\"], strict=False)\n",
    "        self.net_online.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a58e1-52be-4dd5-a42f-5f07126a0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# |eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6fc72e-dda0-461b-b44d-7d46f6d591d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
